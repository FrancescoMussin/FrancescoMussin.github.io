---
title: "Monte Carlo methods"
---

In the simplest terms, Monte Carlo methods are the most basic application of the law of large number. In order to compute one quantity, you draw a lot of samples from a random variable whose expected value is known to be the desired quantity. This idea can be most immediately used to compute integrals, but here we'll see different applications to linear algebra.

## Implicit Trace Estimation

The goal is to estimate the trace of a square matrix. This might seem as a trivial task, since if $A\in \mathbb{R}^{n\times n}$, then its trace is just a matter of $n$ additions. However this can be computationally demanding whenever we don't have immediate access to the diagonal elements of $A$.

As an example, take a network with a huge adjacency matrix $A$. Counting how many triangles are in this graph is equivalent to computing $\text{tr}(A^3)$. Clearly when the graph is very big computing the trace in the naive way is not necessarily the best way of going about the problem.

Instead of assuming constant time access to the entries of the matrix, we're going to assume that we can perform the matrix-vector product $x \longmapsto Ax$ at a reasonable cost. These operations are called *matvecs* for short. This assumption would allow us to get the elements of our matrix at a reasonable cost, simply note that $e_i^T A e_j = A_{ij}$ for all $i,j\in [n]$, where $e_i$ denotes the $i$th vector of $\mathbb{R}^n$'s canonical basis. Thus, even with being able to compute matvecs at a scalable speed, we still can compute the trace of $A$ in $O(n)$ time. However when $n$ is very large this might not be good enough.

The solution is to use randomness.

### The Idea

Isotropic distribution

:   A distribution $\mu$ on $\mathbb{R}^n$ is said to be *isotropic* if given $x\sim \mu$, it holds $\mathbb{E}[x x^T] = I_n$.

Examples of such distributions include:

-   The standard normal distribution on $\mathbb{R}^n$. If $x \sim N(0,I_n)$, then $(xx^T)_{ij} = x_i x_j$, so taking the expected value we get the expected value of $\chi_1^2$ on the diagonal which is $1$, and the expected value of the product of two independent standard normals outside the diagonal, which amounts to $0$.

-   The uniform distribution on $\{\pm 1\}^n$. Realizing this comes down to very similar steps to the ones made for the normal distribution.

-   The uniform distribution on the sphere on the origin and of radius $\sqrt{n}$.

Now, observe that if $x\in \mathbb{R}^n$ is a random vector from an isotropic distribution, then the random variable $Y = x^T A x$ has an interesting property, namely $\mathbb{E}[Y] = \text{tr}(A)$. To see this, first note that $Y = x^T A x = \text{tr}(x^TAx)$, and that due to the cyclic properties of the trace, we can even write $Y = \text{tr}(Ax x^T)$. Having this inside an expected value isn't much of a big deal, as trace and expectation commute (just write down what the trace is and you see this immediately), so

$$
\mathbb{E}[Y] = \mathbb{E}[\text{tr}(x^TAx)] = \mathbb{E}[\text{tr}(Axx^T)] = \text{tr}(\mathbb{E}[A x x^T]) = \text{tr}(A\mathbb{E}[x x^T]) = \text{tr}(A).
$$

With this, we can just take a number $s$ of samples $x_1,\dots,x_s$ from the isotropic distribution, compute $y_i = x_i^T A x_i$, and take the mean:

$$
\widehat{\text{tr}}_s(A) = \frac{1}{s} \sum_{i = 1}^s y_i = \frac{1}{s} \sum_{i = 1}^s x_i^T A x_i.
$$

Taking the expectation of $\widehat{\text{tr}}_s(A)$ would yield $\text{tr}(A)$ because of linearity, so this is an unbiased estimator of the trace. Concerning the variance, we have:

$$
\text{Var}[\widehat{\text{tr}}_s(A)] = \frac{1}{s} \text{Var}[x^T A x],
$$

so our immediate unbiased estimator of the variance is:

$$
\widehat{\nu}_s(A) = \frac{1}{s(s-1)} \sum_{i = 1}^s (y_i - \widehat{\text{tr}}_s(A))^2
$$

### Bounds on the error

As of this moment, we have no idea of the number $s$ of samples needed to get a useful estimate of the trace. To this end, we can rely on statistics. As it turns out, if the distribution of $x$ has four finite moments, meaning $\| x\|_2^4$ has finite expectation, then:

$$
\mathbb{E}[\widehat{\nu}_s(A)] = \text{Var}[\widehat{\text{tr}}_s(A)]
$$
