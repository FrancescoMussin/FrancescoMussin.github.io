[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francesco Mussin",
    "section": "",
    "text": "Welcome to my personal site. I put a number of things in here, including my projects, my personal notes on various topics, and my hobbies.\n\n\nI’m a mathematics students in the Master’s degree program at ETH Zurich.\n\n\n\n\nI’m currently studying at ETH Zurich\nI’ve graduated in Mathematics at Università degli Studi di Milano-Bicocca\nI’ve received my high school diploma in Milan at Liceo Scientifico Piero Bottoni"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "personal_notes.html",
    "href": "personal_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Throughout my academic carreer I have been making LaTeX notes on essentially every course I took. This was a way to store everything I needed to know digitally. Too much later I realized that the best way to do that was with a site, so here we are: no more phisical notebooks or tex files that aren’t stored on all my devices, here I can access everything I want."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Francesco Mussin",
    "section": "",
    "text": "I’m a mathematics students in the Master’s degree program at ETH Zurich."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Francesco Mussin",
    "section": "",
    "text": "I’m currently studying at ETH Zurich\nI’ve graduated in Mathematics at Università degli Studi di Milano-Bicocca\nI’ve received my high school diploma in Milan at Liceo Scientifico Piero Bottoni"
  },
  {
    "objectID": "InformationTheoryNotes/summary.html",
    "href": "InformationTheoryNotes/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "InformationTheoryNotes/intro.html",
    "href": "InformationTheoryNotes/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "InformationTheoryNotes/index.html",
    "href": "InformationTheoryNotes/index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nHello! Welcome to the notes on Information Theory. These are based on the course taught at ETH by professor Lapidoth but they may include topics outside that course in the future.\nDO NOT cite this as a reference for any reason."
  },
  {
    "objectID": "InformationTheoryNotes/references.html",
    "href": "InformationTheoryNotes/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "StatisticalLearning/svm.html",
    "href": "StatisticalLearning/svm.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "In this chapter we’ll get to examine the support vector machine (SVM), which is an effective way to do classification developed in the 90s. These classifiers are considered to be the best “out of the box” classifiers, meaning that they typically don’t need much fine tuning or tweaking. The original idea for this approach for classification is quite simple and intuitive.\nWe’ll mainly focus on binary classification, later we’ll see how to extend what we do for the general case of more than two classes, so we’re taking the set \\(\\mathcal{Y}\\) to have cardinality \\(|\\mathcal{Y}| = 2\\). For simplicity we take \\(\\mathcal{Y} = \\{-1,+1\\}\\).\nAs usual, we’re going to assume that we have a sample \\(\\{z_1,\\dots,z_m\\}\\subseteq \\mathcal{X}\\times \\mathcal{Y}\\) of pairs \\(z_i = (x_i,y_i)\\) distributed i.i.d. according to some distribution on \\(\\mathcal{X}\\times \\mathcal{Y}\\). We’re going to denote:\n\\[\n\\mathcal{I}^+ = \\{i \\in [m]:\\ y_i = 1\\},\\ \\quad \\text{and}\\quad \\mathcal{I}^- = \\{i \\in [m]:\\ y_i = -1\\}.\n\\]"
  },
  {
    "objectID": "StatisticalLearning/svm.html#maximal-margin-classifier",
    "href": "StatisticalLearning/svm.html#maximal-margin-classifier",
    "title": "Support Vector Machines",
    "section": "Maximal Margin Classifier",
    "text": "Maximal Margin Classifier\nThe starting idea behind all of this is to separate the vectors with different labels with a hyperplane. For every \\(w \\in \\mathbb{R}^n\\) with \\(\\|w\\| = 1\\) and \\(b \\in \\mathbb{R}\\) we denote with \\(\\mathcal{H}(w,b)\\) the hyperplane \\(\\{x \\in \\mathbb{R}^n:\\ w^T x = b\\}\\). We call the real number \\(b\\) “offset from the origin” (it corresponds to the distance of the hyperplane to the origin). Every such hyperplane \\(\\mathcal{H}(w,b)\\) yields the binary classifier \\(x \\longmapsto \\text{sign}(w^Tx-b)\\)."
  },
  {
    "objectID": "StatisticalLearning/svm.html#support-vector-classifier",
    "href": "StatisticalLearning/svm.html#support-vector-classifier",
    "title": "Support Vector Machines",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier"
  },
  {
    "objectID": "StatisticalLearning/svm.html#support-vector-machines",
    "href": "StatisticalLearning/svm.html#support-vector-machines",
    "title": "Support Vector Machines",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines"
  },
  {
    "objectID": "StatisticalLearning/references.html",
    "href": "StatisticalLearning/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "StatisticalLearning/index.html",
    "href": "StatisticalLearning/index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nWelcome to my notes on Statistical Learning."
  },
  {
    "objectID": "RandNLA/references.html",
    "href": "RandNLA/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "RandNLA/index.html",
    "href": "RandNLA/index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "InformationTheoryNotes/quantifying_information.html",
    "href": "InformationTheoryNotes/quantifying_information.html",
    "title": "Quantifying Information",
    "section": "",
    "text": "In this chapter, we’ll mainly focus on getting to know the quantities that will be the protagonists of our theorems and derivations. Those include Entropy, Relative Entropy/Cross Entropy, Conditional Entropy, Mutual Information and Conditional Mutual Information. Those are apparently “simple” concepts that in order to be fully understood require quite a deep understanding."
  },
  {
    "objectID": "InformationTheoryNotes/quantifying_information.html#entropy",
    "href": "InformationTheoryNotes/quantifying_information.html#entropy",
    "title": "Quantifying Information",
    "section": "Entropy",
    "text": "Entropy\nMathematically, what is information? Well, if you imagine mathematics as a “divine” computer that figures out everything that follows from logic, the only thing that it cannot predict is randomness; if you werre to tell someone a mathematical result, like \\(3\\cdot 3 = 9\\) or \\(\\dim \\mathbb{R}^n = n\\), you’re technically not conveying any new information, since it is something that, on paper, your receiver could figure out. However, if you tell someone the outcome of a random variable, then you’re conveying some “information” that is unattainable in any other way.\n\nMathematically, information is the outcome of a random variable\n\nHowever, not all outcomes are the same: if I knew that \\(X \\sim \\delta_0\\) had take outcome \\(X = 0\\), I wouldn’t be surprised at all, while if \\(X \\sim \\text{Pois}(1)\\), we should be surprised when \\(X = 100\\). In other words, we need to measure the outcomes of random variables based on how much “surprise” they give use. Here’s another way of seeing it. Remember that when you’re in a probability space \\((\\Omega, \\mathcal{F},\\mathbb{P})\\) and an event \\(A\\in \\mathcal{F}\\) happens, then you have to adjust the probability space to \\((A,\\mathcal{F}|_{A}, \\mathbb{P}(\\ \\cdot \\ |A))\\). A simple way to measure how much this space changes from the original is to compute \\(\\mathbb{P}(A)\\). The bigger this is, the less we have to change our notion of probability, because it means that most of \\(\\mathbb{P}\\)’s mass sits on \\(A\\), but if \\(\\mathbb{P}(A)\\) is small, then it means that you have to cut off a lot of mass from \\(\\mathbb{P}\\), potentially making \\(\\mathbb{P}(\\ \\cdot \\ |A)\\) behave a lot differently than \\(\\mathbb{P}(\\ \\cdot \\ )\\) on most sets.\nSo, in principle, we could just measure \\(\\mathbb{P}(A)\\) to convey the information of the event \\(A\\), however, there is a more useful way to go about this. Imagine you have to find an element within a finite set only by being able to ask yes/no questions. If you want to find it efficiently, then you would like to pose each question in such a way that both a “yes” and a “no” divide the set in half. In general, if you want to find an element within a finite set only by being able to ask questions with \\(b \\geq 2\\) answers, then you should pose each question in such a way that each answer divides the space in \\(b\\) equal parts. The idea of measuring information of an event when a base \\(b\\) is fixed is the amount of questions with \\(b\\) answers that you have to ask to narrow the probability space down to that event. Quantifying this results in: \\[\n\\mathbb{P}(A) = \\left(\\frac{1}{b}\\right)^{I_b(A)}\\mathbb{P}(\\Omega)\n\\] or, by simplifying \\(\\mathbb{P}(\\Omega) = 1\\) and solving for \\(I_b(A)\\): \\[\nI_b(A) = \\log_b\\frac{1}{\\mathbb{P}(A)} = - \\log_b \\mathbb{P}(A)\n\\] Now, the choice of \\(b\\) is up to us, but its pourpouse is to tell us how much each question that we pose divides our space. Typically we use base \\(b = 2\\), in the sense that we answer yes/no type questions. Nonetheless it is useful to develop our theory for an arbitrary base \\(b\\). Thus, for a discrete random variable \\(X\\) taking values on a set \\(\\mathcal{X}\\), we have: \\[\nI_b(X = x) = -\\log_b\\mathbb{P}(X = x).\n\\] Now, if an event has probability \\(0\\) then the above is problematic. If we accept to extend \\(\\log(0) = -\\infty\\) then that event would yield infinite information. In a sense, those events are unattainable with a finite number of questions.\\ \\ Looking more closely into random variables, suppose someone gave you an outcome of \\(X \\sim P_X\\). How much information should we expect? The answer is: \\[\n\\mathbb{E}\\left[-\\log_b(P_X(X))\\right] = -\\sum_{x \\in \\mathcal{X}} P_X(x)\\log_bP_X(x).\n\\]This is the entropy (measured in base \\(b\\)) of \\(X\\), and it is the number of questions with \\(b\\) answers that you have to pose on average before figuring out the value of \\(X\\). In this light, it’s pretty clear that this quantity is always not more than \\(\\log_b|\\mathcal{X}|\\), as you can always do \\(b\\)-ary search on \\(\\mathcal{X}\\) and figure out its value. Again, there is a problem with outcomes of probability \\(0\\), however when we multiply \\(\\log(0) = -\\infty\\) (at this point we give for granted that we extend this function) by \\(0\\) this problem disappears. The reason of this can be found within measure theory, which is the context in which probability spaces are defined. In short, in measure theory there’s a need to extend the real numbers to \\([-\\infty,+\\infty]\\), and to make integrals work, there’s the convention that \\(0 \\cdot (+\\infty) = 0 \\cdot (-\\infty) = 0\\), since any integral over a set of measure zero is taken to be \\(0\\). Applying this convention we see that we don’t run into any issues.\nWe are ready to define entropy.\n\nDefinition: Entropy of a random variable\nGiven a finite set \\(\\mathcal{X}\\), a random variable \\(X:\\Omega\\longrightarrow \\mathcal{X}\\), with probability mass function \\(P_X\\), we define the entropy of \\(X\\) (or rather \\(P_X\\)) in base \\(b &gt; 1\\) as the following quantity: \\[\nH(X) = H(P_X) = -\\sum_{x \\in \\mathcal{X}} P_X(x) \\log P_X(x),\n\\] keeping in mind the convention that \\(0 \\cdot \\infty = 0\\).\n\nWe specified for correctness that this quantity needs a base \\(b\\) in order for it to be a number, but in practice it is something that we omit, since we always use:\n\nbase \\(b = 2\\) when dealing with discrete random variables, speaking of bits of information.\nbase \\(b = \\text{e}\\) when dealing with continuous random variables, speaking of nats of information.\n\n\nExample\nLet \\(\\mathcal{X} = \\{H,T\\}\\) (\\(H\\) representing “heads”, and \\(T\\) representing “tails”), and consider the random variable \\(X:\\Omega \\longrightarrow \\mathcal{X}\\) with: \\[\n\\mathbb{P}(X = H) = P_X(H) = p, \\quad \\text{and}\\quad \\mathbb{P}(X = T) = P_X(T) = 1-p,\n\\] where \\(p\\) is a given number in \\([0,1]\\). Then the entropy of this random variable, or rather of its \\(p\\)-dependent distribution is: \\[\nH(X) = p \\log\\frac{1}{p}+ (1-p)\\log\\frac{1}{1-p} =-p \\log(p) - (1-p)\\log(1-p).\n\\] First, we observe that \\(H(X)\\) doesn’t change its value if we replace \\(p\\) with \\(1-p\\). This is a hint towards a general fact: “Entropy doesn’t care about lables”; it doesn’t matter if the coin is mor or leass likely to land on heads rather than taiils, onlly how likely it is to land on one of those."
  },
  {
    "objectID": "RandNLA/intro.html",
    "href": "RandNLA/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "RandNLA/summary.html",
    "href": "RandNLA/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "StatisticalLearning/intro.html",
    "href": "StatisticalLearning/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "StatisticalLearning/summary.html",
    "href": "StatisticalLearning/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "RandNLA/projects.html",
    "href": "RandNLA/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to my project portfolio! Here are some of the things I’ve been working on.\n\n\n\nGoal:\nTech: Python, TensorFlow, NLP\nStatus: In progress 🚧\n🔗 View Project"
  },
  {
    "objectID": "RandNLA/projects.html#project-1-phase-space",
    "href": "RandNLA/projects.html#project-1-phase-space",
    "title": "Projects",
    "section": "",
    "text": "Goal:\nTech: Python, TensorFlow, NLP\nStatus: In progress 🚧\n🔗 View Project"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to my project portfolio! Here are some of the things I’ve been working on."
  },
  {
    "objectID": "projects.html#project-1-phase-space",
    "href": "projects.html#project-1-phase-space",
    "title": "Projects",
    "section": "🚀 Project 1: Phase space",
    "text": "🚀 Project 1: Phase space\n\nGoal: Using phase space techniques to understand the effect of climate change in Lombardy\nTech: R, Quarto\nStatus: In progress 🚧\n🔗 View Project"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "🚀 Project 2:",
    "text": "🚀 Project 2:"
  },
  {
    "objectID": "Projects/projects.html",
    "href": "Projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to my project portfolio! Here are some of the things I’ve been working on."
  },
  {
    "objectID": "Projects/projects.html#project-1-phase-space",
    "href": "Projects/projects.html#project-1-phase-space",
    "title": "Projects",
    "section": "🚀 Project 1: Phase space",
    "text": "🚀 Project 1: Phase space\n\nGoal:\nTech: Python, TensorFlow, NLP\nStatus: In progress 🚧\n🔗 View Project"
  },
  {
    "objectID": "Projects/projects.html#project-2",
    "href": "Projects/projects.html#project-2",
    "title": "Projects",
    "section": "🚀 Project 2:",
    "text": "🚀 Project 2:"
  },
  {
    "objectID": "docs/Projects.html",
    "href": "docs/Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to my project portfolio! Here are some of the things I’ve been working on."
  },
  {
    "objectID": "docs/Projects.html#project-1-phase-space",
    "href": "docs/Projects.html#project-1-phase-space",
    "title": "Projects",
    "section": "🚀 Project 1: Phase space",
    "text": "🚀 Project 1: Phase space\n\nGoal:\nTech: Python, TensorFlow, NLP\nStatus: In progress 🚧\n🔗 View Project"
  },
  {
    "objectID": "docs/Projects.html#project-2",
    "href": "docs/Projects.html#project-2",
    "title": "Projects",
    "section": "🚀 Project 2:",
    "text": "🚀 Project 2:"
  },
  {
    "objectID": "RandNLA/monte_carlo.html",
    "href": "RandNLA/monte_carlo.html",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "In the simplest terms, Monte Carlo methods are the most basic application of the law of large number. In order to compute one quantity, you draw a lot of samples from a random variable whose expected value is known to be the desired quantity. This idea can be most immediately used to compute integrals, but here we’ll see different applications to linear algebra."
  },
  {
    "objectID": "RandNLA/monte_carlo.html#implicit-trace-estimation",
    "href": "RandNLA/monte_carlo.html#implicit-trace-estimation",
    "title": "Monte Carlo methods",
    "section": "Implicit Trace Estimation",
    "text": "Implicit Trace Estimation\nThe goal is to estimate the trace of a square matrix. This might seem as a trivial task, since if \\(A\\in \\mathbb{R}^{n\\times n}\\), then its trace is just a matter of \\(n\\) additions. However this can be computationally demanding whenever we don’t have immediate access to the diagonal elements of \\(A\\).\nAs an example, take a network with a huge adjacency matrix \\(A\\). Counting how many triangles are in this graph is equivalent to computing \\(\\text{tr}(A^3)\\). Clearly when the graph is very big computing the trace in the naive way is not necessarily the best way of going about the problem.\nInstead of assuming constant time access to the entries of the matrix, we’re going to assume that we can perform the matrix-vector product \\(x \\longmapsto Ax\\) at a reasonable cost. These operations are called matvecs for short. This assumption would allow us to get the elements of our matrix at a reasonable cost, simply note that \\(e_i^T A e_j = A_{ij}\\) for all \\(i,j\\in [n]\\), where \\(e_i\\) denotes the \\(i\\)th vector of \\(\\mathbb{R}^n\\)’s canonical basis. Thus, even with being able to compute matvecs at a scalable speed, we still can compute the trace of \\(A\\) in \\(O(n)\\) time. However when \\(n\\) is very large this might not be good enough.\nThe solution is to use randomness.\n\nThe Idea\n\nIsotropic distribution\n\nA distribution \\(\\mu\\) on \\(\\mathbb{R}^n\\) is said to be isotropic if given \\(x\\sim \\mu\\), it holds \\(\\mathbb{E}[x x^T] = I_n\\).\n\n\nExamples of such distributions include:\n\nThe standard normal distribution on \\(\\mathbb{R}^n\\). If \\(x \\sim N(0,I_n)\\), then \\((xx^T)_{ij} = x_i x_j\\), so taking the expected value we get the expected value of \\(\\chi_1^2\\) on the diagonal which is \\(1\\), and the expected value of the product of two independent standard normals outside the diagonal, which amounts to \\(0\\).\nThe uniform distribution on \\(\\{\\pm 1\\}^n\\). Realizing this comes down to very similar steps to the ones made for the normal distribution.\nThe uniform distribution on the sphere on the origin and of radius \\(\\sqrt{n}\\).\n\nNow, observe that if \\(x\\in \\mathbb{R}^n\\) is a random vector from an isotropic distribution, then the random variable \\(Y = x^T A x\\) has an interesting property, namely \\(\\mathbb{E}[Y] = \\text{tr}(A)\\). To see this, first note that \\(Y = x^T A x = \\text{tr}(x^TAx)\\), and that due to the cyclic properties of the trace, we can even write \\(Y = \\text{tr}(Ax x^T)\\). Having this inside an expected value isn’t much of a big deal, as trace and expectation commute (just write down what the trace is and you see this immediately), so\n\\[\n\\mathbb{E}[Y] = \\mathbb{E}[\\text{tr}(x^TAx)] = \\mathbb{E}[\\text{tr}(Axx^T)] = \\text{tr}(\\mathbb{E}[A x x^T]) = \\text{tr}(A\\mathbb{E}[x x^T]) = \\text{tr}(A).\n\\]\nWith this, we can just take a number \\(s\\) of samples \\(x_1,\\dots,x_s\\) from the isotropic distribution, compute \\(y_i = x_i^T A x_i\\), and take the mean:\n\\[\n\\widehat{\\text{tr}}_s(A) = \\frac{1}{s} \\sum_{i = 1}^s y_i = \\frac{1}{s} \\sum_{i = 1}^s x_i^T A x_i.\n\\]\nTaking the expectation of \\(\\widehat{\\text{tr}}_s(A)\\) would yield \\(\\text{tr}(A)\\) because of linearity, so this is an unbiased estimator of the trace. Concerning the variance, we have:\n\\[\n\\text{Var}[\\widehat{\\text{tr}}_s(A)] = \\frac{1}{s} \\text{Var}[x^T A x],\n\\]\nso our immediate unbiased estimator of the variance is:\n\\[\n\\widehat{\\nu}_s(A) = \\frac{1}{s(s-1)} \\sum_{i = 1}^s (y_i - \\widehat{\\text{tr}}_s(A))^2\n\\]\n\n\nBounds on the error\nAs of this moment, we have no idea of the number \\(s\\) of samples needed to get a useful estimate of the trace. To this end, we can rely on statistics. As it turns out, if the distribution of \\(x\\) has four finite moments, meaning \\(\\| x\\|_2^4\\) has finite expectation, then:\n\\[\n\\mathbb{E}[\\widehat{\\nu}_s(A)] = \\text{Var}[\\widehat{\\text{tr}}_s(A)]\n\\]"
  },
  {
    "objectID": "personal_notes.html#notes-from-courses-from-my-bachelors-studies",
    "href": "personal_notes.html#notes-from-courses-from-my-bachelors-studies",
    "title": "Notes",
    "section": "Notes from courses from my bachelor’s studies",
    "text": "Notes from courses from my bachelor’s studies\nThis is a collection of LaTeX notes taken from courses taught at UniMiB when I was a mathematics Bachelor’s student. They are only available in italian as that was the language of instruction of the material.\n\n📖 Analisi Complessa\n📖 Teoria della Probabilità\n📖 La mia Tesi Triennale\n📖 La presentazione della mia Tesi Triennale"
  },
  {
    "objectID": "personal_notes.html#notes-from-courses-from-my-masters-studies",
    "href": "personal_notes.html#notes-from-courses-from-my-masters-studies",
    "title": "Notes",
    "section": "Notes from courses from my Master’s studies",
    "text": "Notes from courses from my Master’s studies\nThis is a collection of LaTeX notes taken from courses taught at ETHZ during my attendance.\n\n📖 Read my notes on Information Theory (under construction)\n📖 Read my notes on Statistical Learning (under construction)"
  },
  {
    "objectID": "personal_notes.html#other-notes",
    "href": "personal_notes.html#other-notes",
    "title": "Notes",
    "section": "Other notes",
    "text": "Other notes\nThese are just notes on things I’m learning about.\n\n📖 Read my notes on Randomized Numerical Linear Algebra (under construction)"
  },
  {
    "objectID": "Transcriptions/posts/welcome/index.html",
    "href": "Transcriptions/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "Transcriptions/index.html",
    "href": "Transcriptions/index.html",
    "title": "Transcriptions",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 13, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#project-1-phase-space-techniques-for-climate-change",
    "href": "projects.html#project-1-phase-space-techniques-for-climate-change",
    "title": "Projects",
    "section": "🚀 Project 1: Phase space techniques for climate change",
    "text": "🚀 Project 1: Phase space techniques for climate change\n\nGoal: Using phase space techniques to understand the effect of climate change in Lombardy\nTech: R, Quarto\nStatus: In progress 🚧\n🔗 View Project"
  },
  {
    "objectID": "Transcriptions/about.html",
    "href": "Transcriptions/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Transcriptions/posts/post-with-code/index.html",
    "href": "Transcriptions/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]